<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>360 Viewer with Gyro & Optical Flow</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; font-family: sans-serif; }
        #canvas-container { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 10; }
        
        /* The video is hidden visually but active for processing */
        #input-video { 
            position: absolute; top: 0; left: 0; 
            width: 320px; height: 240px; 
            z-index: 1; opacity: 0; pointer-events: none;
        }

        #ui-layer {
            position: absolute; top: 10px; left: 10px; z-index: 20;
            color: lime; text-shadow: 1px 1px 2px black; pointer-events: none;
        }
        
        .error-msg { color: red; background: rgba(0,0,0,0.7); padding: 5px; margin-top: 5px; }
        .status-msg { background: rgba(0,0,0,0.5); padding: 5px; }

        #start-btn {
            position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);
            z-index: 30; padding: 15px 30px; font-size: 18px;
            background: white; border: none; cursor: pointer; border-radius: 5px;
        }
    </style>
    <!-- Three.js for 360 view -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <!-- OpenCV.js for Tracking Fallback -->
    <script async src="https://docs.opencv.org/4.5.4/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script>
</head>
<body>

    <div id="ui-layer">
        <div id="status" class="status-msg">Loading OpenCV...</div>
        <div id="errors"></div>
    </div>

    <button id="start-btn" onclick="startExperience()">Tap to Start</button>

    <video id="input-video" playsinline muted autoplay></video>
    <div id="canvas-container"></div>

<script>
    // --- Global Variables ---
    let camera, scene, renderer, sphere;
    let isGyroActive = false;
    let cvReady = false;
    let videoElement = document.getElementById('input-video');
    
    // Tracking Variables
    let cap, oldFrame, oldPoints, newPoints, frameGray, oldGray;
    let streaming = false;
    let sensitivity = 0.05; // Speed of rotation via camera

    function logError(msg) {
        const d = document.createElement('div');
        d.className = 'error-msg';
        d.innerText = msg;
        document.getElementById('errors').appendChild(d);
        console.error(msg);
    }

    function updateStatus(msg) {
        document.getElementById('status').innerText = msg;
    }

    function onOpenCvReady() {
        cvReady = true;
        updateStatus("System Ready. Press Start.");
    }

    // --- 1. Three.js Setup (The 360 View) ---
    function initThreeJS() {
        const container = document.getElementById('canvas-container');
        
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        
        // Invert camera inside sphere to see texture
        camera.scale.x = -1; 

        renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        container.appendChild(renderer.domElement);

        const geometry = new THREE.SphereGeometry(500, 60, 40);
        
        // Load the image
        const loader = new THREE.TextureLoader();
        loader.crossOrigin = "anonymous"; // Attempt to handle CORS
        
        // URL provided in prompt
        const imageUrl = 'https://c1.wallpaperflare.com/preview/926/262/443/astrophotography-death-valley-national-park-photo-nevada.jpg';
        
        loader.load(imageUrl, (texture) => {
            const material = new THREE.MeshBasicMaterial({ map: texture });
            sphere = new THREE.Mesh(geometry, material);
            scene.add(sphere);
            updateStatus("Image Loaded. Initializing controls...");
        }, undefined, (err) => {
            logError("Error loading image (CORS block likely). Try a local image or a different host.");
        });

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        animate();
    }

    function animate() {
        requestAnimationFrame(animate);
        renderer.render(scene, camera);
    }

    // --- 2. Main Entry Point ---
    async function startExperience() {
        document.getElementById('start-btn').style.display = 'none';
        initThreeJS();

        // Check for Gyroscope permissions (iOS 13+ requires request)
        if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
            try {
                const response = await DeviceOrientationEvent.requestPermission();
                if (response === 'granted') {
                    setupGyro();
                } else {
                    logError("Gyro permission denied. Switching to Camera Tracking.");
                    setupCameraTracking();
                }
            } catch (e) {
                logError(e);
                setupCameraTracking();
            }
        } else {
            // Non-iOS or older devices
            setupGyro();
            // We set a timeout to check if gyro data is actually flowing, if not, fallback
            setTimeout(() => {
                if(!isGyroActive) {
                    updateStatus("No Gyro data detected. Attempting Camera Fallback...");
                    setupCameraTracking();
                }
            }, 2000);
        }
    }

    // --- 3. Method 1: Gyroscope ---
    function setupGyro() {
        window.addEventListener('deviceorientation', (event) => {
            if (!event.alpha && !event.beta) return; // No data
            isGyroActive = true;
            updateStatus("Control: Gyroscope");

            // Convert degrees to radians and update camera rotation
            // Simplified mapping. A full VR control usually needs Quaternion math.
            const lat = Math.max(-85, Math.min(85, 90 - event.beta));
            const lon = event.alpha ? event.alpha : 0;

            const phi = THREE.Math.degToRad(90 - lat);
            const theta = THREE.Math.degToRad(lon);

            const targetX = 500 * Math.sin(phi) * Math.cos(theta);
            const targetY = 500 * Math.cos(phi);
            const targetZ = 500 * Math.sin(phi) * Math.sin(theta);

            camera.lookAt(targetX, targetY, targetZ);
        });
    }

    // --- 4. Method 2: Camera Optical Flow (Fallback) ---
    function setupCameraTracking() {
        if (!cvReady) {
            logError("OpenCV not ready yet.");
            return;
        }

        updateStatus("Control: Camera Tracking (Please allow camera access)");

        const constraints = {
            video: {
                facingMode: 'environment', // Back camera if available
                width: 320,
                height: 240
            },
            audio: false
        };

        navigator.mediaDevices.getUserMedia(constraints)
            .then(function(stream) {
                videoElement.srcObject = stream;
                videoElement.play();
                videoElement.onloadedmetadata = function() {
                    streaming = true;
                    initTracking();
                };
            })
            .catch(function(err) {
                logError("Camera Error: " + err.name + " " + err.message);
            });
    }

    function initTracking() {
        // Setup OpenCV variables
        cap = new cv.VideoCapture(videoElement);
        
        oldFrame = new cv.Mat(videoElement.height, videoElement.width, cv.CV_8UC4);
        oldGray = new cv.Mat();
        
        // Read first frame
        cap.read(oldFrame);
        cv.cvtColor(oldFrame, oldGray, cv.COLOR_RGBA2GRAY);

        // Define the 9 points grid (3x3)
        // Video is 320x240. We want a grid in the center.
        oldPoints = new cv.Mat(9, 1, cv.CV_32FC2);
        generateGridPoints(oldPoints);

        // Arrays for calculation
        frameGray = new cv.Mat();
        newPoints = new cv.Mat();
        let status = new cv.Mat();
        let err = new cv.Mat();
        
        // Window size for Lucas-Kanade (12x12 as requested in prompt, though passed as 12,12 size)
        let winSize = new cv.Size(12, 12); 
        let maxLevel = 2;
        let criteria = new cv.TermCriteria(cv.TermCriteria_EPS | cv.TermCriteria_COUNT, 10, 0.03);

        updateStatus("Control: Camera Active. Move device to pan.");

        function processVideo() {
            if (!streaming) return;

            try {
                let begin = Date.now();

                // Read new frame
                let frame = new cv.Mat(videoElement.height, videoElement.width, cv.CV_8UC4);
                cap.read(frame);
                cv.cvtColor(frame, frameGray, cv.COLOR_RGBA2GRAY);

                // Calculate Optical Flow
                cv.calcOpticalFlowPyrLK(oldGray, frameGray, oldPoints, newPoints, status, err, winSize, maxLevel, criteria);

                // Calculate average movement
                let deltaX = 0;
                let deltaY = 0;
                let pointsFound = 0;
                let resetNeeded = false;

                // Loop through 9 points
                for (let i = 0; i < status.rows; i++) {
                    if (status.data[i] === 1) { // If point was successfully tracked
                        let pt1x = oldPoints.data32F[i * 2];
                        let pt1y = oldPoints.data32F[i * 2 + 1];
                        let pt2x = newPoints.data32F[i * 2];
                        let pt2y = newPoints.data32F[i * 2 + 1];

                        deltaX += (pt2x - pt1x);
                        deltaY += (pt2y - pt1y);
                        pointsFound++;

                        // Logic: "when the tracking is going out of the screen, track a new grid"
                        // Video boundaries: 0 to 320 (width), 0 to 240 (height)
                        // If any point is within 10px of edge, trigger reset
                        if(pt2x < 10 || pt2x > 310 || pt2y < 10 || pt2y > 230) {
                            resetNeeded = true;
                        }
                    }
                }

                // If not enough points found or points moved out of bounds
                if (pointsFound < 4 || resetNeeded) {
                   // Generate new grid at center
                   generateGridPoints(newPoints); // Reset "new" to center
                   // We treat this frame as a "jump" so we don't move the camera this specific frame
                   // to avoid jerky resetting
                } else {
                    // Apply movement to Three.js Camera
                    // Average the delta
                    let avgDx = deltaX / pointsFound;
                    let avgDy = deltaY / pointsFound;

                    // Invert controls: Camera moves left (image features move right), we want to look left.
                    // Usually: Image moves Right -> Camera turned Left.
                    // Prompt: "camera feed moves to left move the image also to left"
                    // If I pan phone left, pixels in video move RIGHT.
                    // So if pixels move Right (+x), we rotate camera Left.
                    
                    camera.rotation.y += avgDx * sensitivity * 0.1; 
                    camera.rotation.x += avgDy * sensitivity * 0.1;
                }

                // Swap frames and points for next iteration
                frameGray.copyTo(oldGray);
                newPoints.copyTo(oldPoints);
                frame.delete();

                requestAnimationFrame(processVideo);

            } catch (err) {
                logError("Tracking Error: " + err);
            }
        }

        requestAnimationFrame(processVideo);
    }

    // Helper to generate the 9 points grid (3x3) in center of video
    function generateGridPoints(mat) {
        // Video is 320x240
        // Center is 160, 120
        // We want a grid. Let's space them 30px apart.
        const centerX = 160;
        const centerY = 120;
        const spacing = 40;

        let index = 0;
        for(let x = -1; x <= 1; x++) {
            for(let y = -1; y <= 1; y++) {
                mat.data32F[index * 2] = centerX + (x * spacing);
                mat.data32F[index * 2 + 1] = centerY + (y * spacing);
                index++;
            }
        }
    }

</script>
</body>
</html>
