<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>360 Viewer + Visual Tracking</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #111; font-family: sans-serif; }
        
        /* 360 View Container */
        #canvas-container { 
            position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 1; 
        }

        /* Camera Preview Area (Half Screen) */
        #camera-wrapper {
            position: absolute; 
            bottom: 0; left: 0; 
            width: 100%; height: 50%; /* Half screen height */
            z-index: 10;
            background: rgba(0,0,0,0.5);
            display: none; /* Hidden until camera starts */
            pointer-events: none; /* Let touches pass through */
        }
        
        #input-video {
            width: 100%; height: 100%; 
            object-fit: cover; 
            opacity: 0.8;
        }

        /* Overlay canvas for drawing the tracking grids */
        #debug-canvas {
            position: absolute; top: 0; left: 0; 
            width: 100%; height: 100%;
        }

        /* UI / Messages */
        #ui-layer {
            position: absolute; top: 10px; left: 10px; z-index: 20;
            color: lime; text-shadow: 1px 1px 2px black; font-size: 14px;
            pointer-events: none;
        }

        .error-msg { 
            color: #ff4444; background: rgba(0,0,0,0.8); 
            padding: 8px; margin-top: 5px; border-left: 3px solid red;
        }
        
        .status-msg { background: rgba(0,0,0,0.6); padding: 5px; border-radius: 4px; }

        /* Start Button */
        #start-btn {
            position: absolute; top: 50%; left: 50%; 
            transform: translate(-50%, -50%);
            z-index: 30; padding: 20px 40px; font-size: 20px;
            background: #28a745; color: white; border: none; 
            cursor: pointer; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
    </style>
    <!-- Three.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <!-- OpenCV.js -->
    <script async src="https://docs.opencv.org/4.5.4/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script>
</head>
<body>

    <div id="ui-layer">
        <div id="status" class="status-msg">Initializing System...</div>
        <div id="errors"></div>
        <div id="tracking-info"></div>
    </div>

    <button id="start-btn" onclick="startExperience()">TAP TO START</button>

    <div id="canvas-container"></div>

    <!-- Container for Video + Visual Debugging -->
    <div id="camera-wrapper">
        <video id="input-video" playsinline muted autoplay></video>
        <canvas id="debug-canvas"></canvas>
    </div>

<script>
    // --- Configuration ---
    const IMAGE_URL = 'https://c1.wallpaperflare.com/preview/926/262/443/astrophotography-death-valley-national-park-photo-nevada.jpg';
    const TRACKING_GRID_SIZE = 12; // 12x12 pixel grids
    const SENSITIVITY = 0.08;      // Rotation speed
    
    // --- Global Variables ---
    let camera, scene, renderer, sphere;
    let cvReady = false;
    let isTracking = false;
    let videoElement = document.getElementById('input-video');
    let debugCanvas = document.getElementById('debug-canvas');
    let debugCtx = debugCanvas.getContext('2d');

    // OpenCV Variables
    let cap, oldFrame, oldGray, frameGray, oldPoints, newPoints, statusMat, errMat;
    let winSize, maxLevel, criteria;

    function logMsg(msg, isError = false) {
        const el = document.getElementById(isError ? 'errors' : 'status');
        if(isError) {
            const d = document.createElement('div');
            d.className = 'error-msg';
            d.innerText = msg;
            el.appendChild(d);
            console.error(msg);
        } else {
            el.innerText = msg;
            console.log(msg);
        }
    }

    function onOpenCvReady() {
        cvReady = true;
        logMsg("System Ready. Waiting for user input.");
    }

    // --- 1. Three.js Initialization ---
    function initThreeJS() {
        const container = document.getElementById('canvas-container');
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        camera.scale.x = -1; // Invert for inside view

        renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        container.appendChild(renderer.domElement);

        const geometry = new THREE.SphereGeometry(500, 60, 40);
        const loader = new THREE.TextureLoader();
        loader.crossOrigin = "anonymous";
        
        loader.load(IMAGE_URL, (texture) => {
            const material = new THREE.MeshBasicMaterial({ map: texture });
            sphere = new THREE.Mesh(geometry, material);
            scene.add(sphere);
        }, undefined, (err) => {
            logMsg("Error loading image. Check CORS or URL.", true);
        });

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        animate();
    }

    function animate() {
        requestAnimationFrame(animate);
        renderer.render(scene, camera);
    }

    // --- 2. Main Logic ---
    async function startExperience() {
        document.getElementById('start-btn').style.display = 'none';
        
        // Security Check
        if (!window.isSecureContext) {
            logMsg("CRITICAL ERROR: Not running in Secure Context (HTTPS/localhost). Camera access will fail.", true);
        }

        initThreeJS();

        // Try Gyroscope First
        if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
            try {
                const response = await DeviceOrientationEvent.requestPermission();
                if (response === 'granted') {
                    setupGyro();
                } else {
                    logMsg("Gyro permission denied. Starting Camera...", true);
                    setupCamera();
                }
            } catch (e) {
                logMsg("Gyro Error: " + e, true);
                setupCamera();
            }
        } else {
            // Android/Desktop or non-permission iOS
            window.addEventListener('deviceorientation', handleGyro);
            // Wait 1s to see if we actually get data. If not, start camera.
            setTimeout(() => {
                if (!gyroDataReceived) {
                    logMsg("No Gyro data detected. Falling back to Camera...");
                    window.removeEventListener('deviceorientation', handleGyro);
                    setupCamera();
                }
            }, 1000);
        }
    }

    // --- 3. Gyroscope Logic ---
    let gyroDataReceived = false;
    function setupGyro() {
        window.addEventListener('deviceorientation', handleGyro);
    }

    function handleGyro(event) {
        if (event.alpha === null || event.beta === null) return;
        gyroDataReceived = true;
        logMsg("Mode: Gyroscope Control");

        // Simple mapping
        const lat = Math.max(-85, Math.min(85, 90 - event.beta));
        const lon = event.alpha || 0;
        const phi = THREE.Math.degToRad(90 - lat);
        const theta = THREE.Math.degToRad(lon);

        const targetX = 500 * Math.sin(phi) * Math.cos(theta);
        const targetY = 500 * Math.cos(phi);
        const targetZ = 500 * Math.sin(phi) * Math.sin(theta);
        camera.lookAt(targetX, targetY, targetZ);
    }

    // --- 4. Camera & Visual Tracking Logic ---
    function setupCamera() {
        if (!cvReady) {
            logMsg("OpenCV not ready. Refresh page.", true);
            return;
        }

        document.getElementById('camera-wrapper').style.display = 'block';
        logMsg("Mode: Optical Flow Tracking (Camera)");

        const constraints = {
            video: {
                facingMode: 'environment', // Rear camera
                width: { ideal: 320 },
                height: { ideal: 240 }
            }
        };

        navigator.mediaDevices.getUserMedia(constraints)
            .then((stream) => {
                videoElement.srcObject = stream;
                videoElement.play();
                videoElement.onloadedmetadata = () => {
                    // Match canvas size to video render size
                    debugCanvas.width = videoElement.videoWidth;
                    debugCanvas.height = videoElement.videoHeight;
                    initTracking();
                };
            })
            .catch((err) => {
                logMsg("Camera Access Denied: " + err.message, true);
                logMsg("Ensure you are on HTTPS and allowed permissions.", true);
            });
    }

    function initTracking() {
        const width = videoElement.videoWidth;
        const height = videoElement.videoHeight;

        // Init OpenCV Mats
        cap = new cv.VideoCapture(videoElement);
        oldFrame = new cv.Mat(height, width, cv.CV_8UC4);
        oldGray = new cv.Mat();
        frameGray = new cv.Mat();
        oldPoints = new cv.Mat();
        newPoints = new cv.Mat();
        statusMat = new cv.Mat();
        errMat = new cv.Mat();

        // Read first frame
        cap.read(oldFrame);
        cv.cvtColor(oldFrame, oldGray, cv.COLOR_RGBA2GRAY);

        // Parameters for Lucas-Kanade
        winSize = new cv.Size(TRACKING_GRID_SIZE, TRACKING_GRID_SIZE);
        maxLevel = 2;
        criteria = new cv.TermCriteria(cv.TermCriteria_EPS | cv.TermCriteria_COUNT, 10, 0.03);

        generateGrid(oldPoints, width, height);
        isTracking = true;
        processVideo();
    }

    function generateGrid(pointMat, w, h) {
        // Create 3x3 grid centered in the frame
        // This makes 9 points
        let points = [];
        let stepX = 40; 
        let stepY = 40;
        let centerX = w / 2;
        let centerY = h / 2;

        for(let x = -1; x <= 1; x++) {
            for(let y = -1; y <= 1; y++) {
                points.push(centerX + x * stepX);
                points.push(centerY + y * stepY);
            }
        }

        // Resize mat to hold 9 points (float32, 2 channels)
        pointMat.create(9, 1, cv.CV_32FC2);
        for (let i = 0; i < 9; i++) {
            pointMat.data32F[i * 2] = points[i * 2];
            pointMat.data32F[i * 2 + 1] = points[i * 2 + 1];
        }
    }

    function processVideo() {
        if (!isTracking) return;

        try {
            let begin = Date.now();

            // 1. Read new frame
            let frame = new cv.Mat(videoElement.videoHeight, videoElement.videoWidth, cv.CV_8UC4);
            cap.read(frame);
            cv.cvtColor(frame, frameGray, cv.COLOR_RGBA2GRAY);

            // 2. Calc Optical Flow
            if(oldPoints.rows > 0) {
                cv.calcOpticalFlowPyrLK(oldGray, frameGray, oldPoints, newPoints, statusMat, errMat, winSize, maxLevel, criteria);
            }

            // 3. Visualize & Calculate Movement
            debugCtx.clearRect(0, 0, debugCanvas.width, debugCanvas.height);
            
            let deltaX = 0, deltaY = 0, goodPoints = 0;
            let resetNeeded = false;
            let w = videoElement.videoWidth;
            let h = videoElement.videoHeight;

            // Loop through the 9 points
            for (let i = 0; i < statusMat.rows; i++) {
                if (statusMat.data[i] === 1) {
                    let x = newPoints.data32F[i * 2];
                    let y = newPoints.data32F[i * 2 + 1];
                    let oldX = oldPoints.data32F[i * 2];
                    let oldY = oldPoints.data32F[i * 2 + 1];

                    // Draw the 12x12 grid (box)
                    debugCtx.strokeStyle = "lime";
                    debugCtx.lineWidth = 2;
                    debugCtx.strokeRect(x - 6, y - 6, 12, 12);

                    deltaX += (x - oldX);
                    deltaY += (y - oldY);
                    goodPoints++;

                    // Check bounds (reset if going off screen)
                    if (x < 10 || x > w - 10 || y < 10 || y > h - 10) {
                        resetNeeded = true;
                    }
                }
            }

            // 4. Update Camera or Reset Grid
            if (goodPoints < 4 || resetNeeded) {
                // Not enough points or moved too far -> Reset grid to center
                generateGrid(newPoints, w, h);
                debugCtx.strokeStyle = "red";
                debugCtx.strokeRect(w/2 - 20, h/2 - 20, 40, 40); // Flash red center
            } else {
                let avgX = deltaX / goodPoints;
                let avgY = deltaY / goodPoints;

                // Move 360 Camera
                // Note: If video pixels move RIGHT, camera is panning LEFT.
                // We rotate the 3D camera in the opposite direction of pixel flow to mimic head movement.
                camera.rotation.y += avgX * SENSITIVITY; 
                camera.rotation.x += avgY * SENSITIVITY;
            }

            // 5. Cleanup for next frame
            frameGray.copyTo(oldGray);
            newPoints.copyTo(oldPoints);
            frame.delete();

            requestAnimationFrame(processVideo);

        } catch (err) {
            logMsg("Tracking Loop Error: " + err, true);
        }
    }
</script>
</body>
</html>
